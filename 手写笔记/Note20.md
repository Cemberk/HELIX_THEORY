# v2.0版本三测
　　本章对v2.0进行第三轮测试训练，此前一测为2019年5月，二测为2019年12月，本轮改进主要是在上轮二测后，改进了以下几个方面:
  1. 反向反馈类比，更快更全面的抽象。
  2. 以及决策期对网络更全面的使用（主要是SP方面）。
  3. 对短时记忆更全面的支持。
  4. 以及对外循环更好的支持完善。

　　所以本次三测，或有望成为发布v2.0前的最后一轮测试，且在上轮二测中已经基本测通了认知期的代码，以及测了大多数决策期的代码。

***

<!-- TOC -->

- [v2.0版本三测](#v20版本三测)
  - [n20p01 三测: 规划训练1](#n20p01-三测-规划训练1)

<!-- /TOC -->

### n20p01 三测: 规划训练1
`CreateTime 2020.06.06`

| 20011 | 训练步骤细节分析版: | STATUS |
| --- | --- | --- |
| A | 果可吃 | T |
|  | **1. 直投,飞右上,直投** | T |
| B | 远果不能吃 | T |
|  | **1. 重启,远投右,饿** | T |
|  | **2. 重进成长页,远投右** | T |
| C | 学飞 |  |
|  | **1. 重启,远投右,飞右** |  |
| D | 用飞 |  |
|  | **重启,远投上,马上饿** |  |

**20012完成标记**

* A组: 果可吃
  - 直投,飞右上,直投
    ```c
    //类比出无经纬果可吃;
    //1. 正向类比-抽象时序
    STEPKEY------------------------ 正向反馈类比 START ------------------------
    F8[A4(),A5(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0,经199,纬375),A1(吃1),A6()]->M3{64}
    F3[A1(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0),A1(吃1),A2()]->M1{64}
    STEPKEY--->> 构建时序:F11[A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0),A1(吃1)]->M7{64}
    //2. 正向类比-抽象概念
    STEPKEY—> 构建概念:A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0)
    STEPKEY具象1:A5(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0,经199,纬375)
    STEPKEY具象2:A1(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0)
    ```
* B组: 远果不能吃
  - 远投右
    ```c
    //B1-远投 (预测吃可饱);
    STEPKEY时序识别: SUCCESS >>> matchValue:0.500000 F11[A10(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0),A1(吃1)]->M7{64}
    //B1-饿 (真实为饿)
    STEPKEY------------------------ 反向反馈类比 START ------------------------
    STEPKEY----> 反向反馈类比 CreateFo内容:F2[A1(距0),A1(吃1)]->M10{64}
    STEPKEY----> 反向反馈类比 CreateFo内容:F2[A1(距46,经202,纬508)]->M13{-51}
    //B1-进R+,输出吃,但并不能解决问题;
    -> SP_Hav_isOut为TRUE: A1(吃1)

    //B2-远投右 (预测更饿)
    STEPKEY时序识别: SUCCESS >>> matchValue:1.000000 F13[A7(速0,宽5,高5,形2.5,向→,红0,绿255,蓝0,皮0,距54,经191,纬531)]->M8{-51}
    //B2-进R- (进入SP行为化,得到了GLDic,但失败了,因为此时的行为应该是飞近,但没学过飞)
    ------SP_GL行为化:距37 -> 距0
    //B2-进R+ (循环前两帧,一帧越界`参考bug7`,一帧输出吃`同B1`)
    <警告> 行为化概念无效
    -> SP_Hav_isOut为TRUE: A1(吃1)
    ```
* C组: 学飞
  - 远投右,飞右
    ```c
    //C1-飞右 (进入SP-GL行为化)
    ------SP_GL行为化:距52 -> 距0
    //BUG-发现GL找不以索引glAlg,导致行为化失败;
    ```


| 20013_BUG | STATUS |
| --- | --- |
| 1. 活跃度消耗在决策每轮循环调用之前,导致决策循环完未消耗活跃度; | T |
| 2. demand.fo不应期不工作,导致每轮循环联想同一解决方案; | T |
| 3. 训练到C时,远投右,预测时序为mv-,应该进TOP.R-,但却进了P+; | T |
| 4. 训练到B1点击饿时,进了R+,而不是R- (预测为正,实际为负); | T |
| > 正常的,因为预测为正,所以会R+输出`吃` (虽然并吃不到); | T |
| 5. 当决策完成时,是否主动观察自身价值状态 (比如输出吃,观察饥饿状态是否变化); | T |
| > 暂不,因为尽量让imv自然发生,像吃时不会马上饱,而是有味觉吸引着继续; | T |
| 6. TOR.R-中indexOfAbsItem方法得到-1失败的bug | T |
| 7. 有outModel.actionIndex>fo.content.count的问题,导致"行为化概念无效"; | T |
| > 因为B组共三帧,最近一帧右投预测不能吃,R-行为化失败,因为不会飞; | T |
| > 倒数二帧,是B1的马上饿,输出吃,预测可饱,R+行为化失败,因为`吃`已输出,所以越界; | T |
| 8. C1训练右飞两次后,为何还是找不到距离变小索引; |  |
| > 分析原因: 因为在用pAlg索引找,未经历过飞至距0,所以无法找到; |  |
| > 修复方案: 改为由sAlg索引,找s出发,变小接近p; |  |
| > 经查,修改为sAlg依然没有glAlg,所以查内类比是否根本没构建`距变小`节点; |  |
| 9. C1飞右后一帧,被识别为距0果,导致前后内类比成了`距71->0`,这显然不对; |  |
| > 经查因为飞后一帧的速度!=0,所以无法全含,只能纯相似匹配,导致识别为距0果; |  |
