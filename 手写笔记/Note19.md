# 测试 & 细节改动 & 训练

<!-- TOC -->

- [测试 & 细节改动 & 训练](#%E6%B5%8B%E8%AF%95--%E7%BB%86%E8%8A%82%E6%94%B9%E5%8A%A8--%E8%AE%AD%E7%BB%83)
  - [n19p1 规划性训练2](#n19p1-%E8%A7%84%E5%88%92%E6%80%A7%E8%AE%AD%E7%BB%832)
  - [n19p2 TIR_Alg_FromRT算法迭代](#n19p2-tir_alg_fromrt%E7%AE%97%E6%B3%95%E8%BF%AD%E4%BB%A3)
  - [n19p3 反向反馈类比](#n19p3-%E5%8F%8D%E5%90%91%E5%8F%8D%E9%A6%88%E7%B1%BB%E6%AF%94)
  - [n19p4 外类比迭代: 正向反馈类比](#n19p4-%E5%A4%96%E7%B1%BB%E6%AF%94%E8%BF%AD%E4%BB%A3-%E6%AD%A3%E5%90%91%E5%8F%8D%E9%A6%88%E7%B1%BB%E6%AF%94)
  - [n19p5 规划性训练3](#n19p5-%E8%A7%84%E5%88%92%E6%80%A7%E8%AE%AD%E7%BB%833)
  - [n19p6 扩展识别算法支持: MatchType_Seem](#n19p6-%E6%89%A9%E5%B1%95%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E6%94%AF%E6%8C%81-matchtype_seem)
  - [n19p7 RTAlg反思:全面性迭代](#n19p7-rtalg%E5%8F%8D%E6%80%9D%E5%85%A8%E9%9D%A2%E6%80%A7%E8%BF%AD%E4%BB%A3)
  - [n19p8 结合`MC_Value`和`反向类比`分析决策失败的问题](#n19p8-%E7%BB%93%E5%90%88mc_value%E5%92%8C%E5%8F%8D%E5%90%91%E7%B1%BB%E6%AF%94%E5%88%86%E6%9E%90%E5%86%B3%E7%AD%96%E5%A4%B1%E8%B4%A5%E7%9A%84%E9%97%AE%E9%A2%98)
  - [n19p9 用"V+A"来解决反向类比与MC_Value的协作问题](#n19p9-%E7%94%A8va%E6%9D%A5%E8%A7%A3%E5%86%B3%E5%8F%8D%E5%90%91%E7%B1%BB%E6%AF%94%E4%B8%8Emc_value%E7%9A%84%E5%8D%8F%E4%BD%9C%E9%97%AE%E9%A2%98)
  - [n19p10 MC_Alg算法迭代V3](#n19p10-mc_alg%E7%AE%97%E6%B3%95%E8%BF%AD%E4%BB%A3v3)
  - [n19p11 双向任务——决策对mModel全面支持](#n19p11-%E5%8F%8C%E5%90%91%E4%BB%BB%E5%8A%A1%E5%86%B3%E7%AD%96%E5%AF%B9mmodel%E5%85%A8%E9%9D%A2%E6%94%AF%E6%8C%81)
  - [n19p12 回归测训4](#n19p12-%E5%9B%9E%E5%BD%92%E6%B5%8B%E8%AE%AD4)
  - [n19p13 关联强度整理](#n19p13-%E5%85%B3%E8%81%94%E5%BC%BA%E5%BA%A6%E6%95%B4%E7%90%86)
  - [n19p14 决策-SP协作](#n19p14-%E5%86%B3%E7%AD%96-sp%E5%8D%8F%E4%BD%9C)
  - [n19p15 双向任务——决策对mModel全面支持2](#n19p15-%E5%8F%8C%E5%90%91%E4%BB%BB%E5%8A%A1%E5%86%B3%E7%AD%96%E5%AF%B9mmodel%E5%85%A8%E9%9D%A2%E6%94%AF%E6%8C%812)
  - [n19p16 决策四模式从TOP到TOR](#n19p16-%E5%86%B3%E7%AD%96%E5%9B%9B%E6%A8%A1%E5%BC%8F%E4%BB%8Etop%E5%88%B0tor)
  - [n19p17 决策四模式之OutModel短时记忆](#n19p17-%E5%86%B3%E7%AD%96%E5%9B%9B%E6%A8%A1%E5%BC%8F%E4%B9%8Boutmodel%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86)
  - [n19p18 决策四模式之行为化迭代](#n19p18-%E5%86%B3%E7%AD%96%E5%9B%9B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%A1%8C%E4%B8%BA%E5%8C%96%E8%BF%AD%E4%BB%A3)
  - [n19p19 P+模式重新改回递归](#n19p19-p%E6%A8%A1%E5%BC%8F%E9%87%8D%E6%96%B0%E6%94%B9%E5%9B%9E%E9%80%92%E5%BD%92)
  - [n19p20 决策与外循环深度集成OutModel](#n19p20-%E5%86%B3%E7%AD%96%E4%B8%8E%E5%A4%96%E5%BE%AA%E7%8E%AF%E6%B7%B1%E5%BA%A6%E9%9B%86%E6%88%90outmodel)
  - [TODOLIST](#todolist)

<!-- /TOC -->

### n19p1 规划性训练2
`CreateTime 2020.04.06`

> 说明: 继续note18的训练进度;

| 19011 | 训练步骤细节分析版: | STATUS |
| --- | --- | --- |
| **A组** | 目标: `果可吃` | 独立可训 |
| A1,2 | **直投,直投** | T |
| A3 | **重启,移动,并直投** | T |
| **B组** | 目标: `远果不能吃+模糊识别` | 独立可训 |
| B4,5 | **重启,远投,马上饿,马上饿** | T |
| B6 | **重启,远投(投向与A6不同的方向),马上饿** | T |
| B7 | **返回,重进成长演示页,远投** | T |
| **C组** | 目标: `学习飞行,可解决距离的常识+内类比` | 独立可训 |
| C8 | **重启,远投上,摸翅膀上** | T |
| C9 | **再摸...(从各种飞行角度,发现与距离的变化的映射)** | 暂略 |
| **D组** | 目标: `决策,并输出飞行离坚果更近` |  |
| D10 | **重启,远投上,马上饿** | 依赖ABC组 |
|  | =>行为化输出吃,而不是飞,应该从`远果,吃`中学习到,远果不能吃 `BUG_1`; |  |
|  | =>MC_Value到RelativeValue行为化 `BUG_2` |  |
| D11 |  |  |

| BUG | STATUS |
| --- | --- |
| 1. 远投后,MC行为化结果为`吃`,而不是`飞`的问题; |  |
| 2. 下投坚果,行为化为`飞上`,导致更飞远了; |  |
| > 分析: 小鸟只经历过飞上,离上坚果更近,所以这里要加入对方向的判断,抽象出不同方向坚果进行不同方向飞行的知识; |  |

<br><br><br><br>

### n19p2 TIR_Alg_FromRT算法迭代
`CreateTime 2020.04.06`
> 因为在精训D10时,发现M特化值:`经266`,在TIR_Alg_FromRT()时,重组[经266,高5],识别后,得到[距20,经266,高5],其中杂质`距20`导致评价失败;所以本节对TIR_Alg_FromRT算法进行迭代;

> **缩写说明:**
> 1. mModel : 全称AIShortMatchModel

| 19021 | 算法变动说明 |
| --- | --- |
| 1 | 去除fuzzy模糊匹配,仅将match返回; |
| 2 | 返回结果必须包含mUniqueValue特化值; |

| 19022 | 改动后,测试与BUG |
| --- | --- |
| **BUG** | 当mUniqueValue=`距20`,same_ps=`w5,h5,g1`时,rtAlg=`w5,h5,g1,距20`,执行新版识别后,发现根本找不到距20的,全含的坚果,因为在抽象时,距20早已被去除; |
|  | 分析: 此问题关键在于,识别无法成功,我们需要更多抽象的常识,我们可以通过以下两种途径来获取: |
|  | 途径1: 认知阶段,更多确切的常识 (依赖更多经历,进行外类比); |
|  | 途径2: 决策反馈,更多的对确切信息的价值指向; |
| **向性分析** | 综上,我们需要整个系统的角度来分析此问题的解 |
|  | 1. 决策,向性下,从具象中找方案; |
|  | 2. 反思,向性上,从抽象中找评价; |
| **总结** | 本bug最大的原因,是反思时,无法从抽象中,匹配到确切的价值指向,导致评价失败; |
| **疑难点** | 而`距20`这种很具体的信息,在外类比抽象时,又极易被丢失掉,这个矛盾导致bug; |
| **解决** | 那么只要我们可以对导致反价值的信息进行直接抽象,即可解决此问题,故我们可以采取将预测与反馈之间进行类比,来实现此需求,`转至19023`; |

| 19023 | 从系统TO角度,分析抽象不足的问题; |
| --- | --- |
| 决策与反思示图 | ![](assets/242_决策与反思示图.png) |
| 说明 | 绿色: 根据MModel识别与预测,进行反思评价; |

| 19024 | 从系统TI角度,分析抽象不足的问题; |
| --- | --- |
| 认知与反馈示图 | ![](assets/243_认知与反馈示图.png) |
| 说明 | 绿色: 根据MModel识别与预测,进行反馈认知; |
| 说明2 | 反馈imv与预测mModel.mv,根据同反向,产生同向和反向两种反馈类比; |
| 说明3 | 因同向反馈类比与现有的analogy_Outside()类似,故现在我们只写Feedback_Diff(); `转至n19p3` |

<br><br><br><br>

### n19p3 反向反馈类比
`CreateTime 2020.04.06`

**1. 简介:**   
　　在TIP时,假如反馈的imv与mModel的预测不符,会触发反向反馈类比(以下简称反向类比),以快速获取非常确切的抽象常识;本节从此出发,设计此类比算法;  

**2. 曾用名:**
1. 占位规律,参考:n13p15;
2. 找不同,参考:n15p15, n15p17;

| 19031 | 不相符的原因,归咎于两种: |
| --- | --- |
| **第1种** | 该出现的(`概念`/`稀疏码`)未出现; |
| 举例 | 一直能赢,今天乔丹没来,所以输了; |
| 结果 | 乔丹对胜利很重要 `[乔丹]->{mv+}`; |
| **第2种** | 不该出现的(`概念`/`稀疏码`)出现; |
| 举例 | 一直能赢,今天尼古拉赵四加入,我们输了; |
| 结果 | 赵四对失败有责任 `[赵四]->{mv-}`; |

```objective-c
//19032 伪代码;
+(void) analogy_Feedback_Diff:(AIFoNodeBase*)mFo p:(AIFoNodeBase*)pFo{
    if(不符合预测){ //指mMv和pMv同类型不同向 (一正一负);
        /** 第1级: 类比fo.content_ps;
          *  M:[abcd]->{mmv+}
          *  P:[a3bcxyz]->{pmv-}
          *  ms:[d]->{mmv+/4}; (价值影响权重=ms数/M数);
          *  ps:[3xyz]->{pmv-*4/7}; (价值影响权重=ps数/P数);
          */

        /** 第2级: 类比alg.content_ps;
          * M:[(w1),(d5,h6)]->{mmv+};
          * P:[a,(w1,b2),(d5,h3)]->{pmv-};
          * ms:[(h6)]->{mmv+/2};    //说明h6很重要
          * ps:[a,(b2),(h3)]->{pmv-*3/3} //说明a,b2,h3很多余
          */

        //3. 构建: 概念,时序和MV;
    }
}
```
***

| 19033 | TODO | STATUS |
| --- | --- | --- |
| 1 | 瞬时记忆中放match而不是proto (放了matchCache和protoCache两种); | T |
| 2 | 为Feedback算法的结果,制定新的时序类型 (同时新类型的refPorts) (可考虑先由ds/at来做类型,不改refPorts,取指定类型的refPorts时可由指针标识进行类型筛选); |  |
| 3 | 将反向类比构建器中,ms导致pmv改为mmv; | T |
| 4 | 将反向反馈类比导致价值影响量,改为越确切,影响越大 (即平均担责),公式: 担责量=`总价值/有责任元素数`; | T |

| 19034 | 测试 |
| --- | --- |
| 关键问题 | 因反向类比的调用频率太低`参考19041`,故不能单靠此来解决的抽象不足问题`参考19022`; |
| 解决方式 | 对正向类比迭代 `转n19p4` |

<br><br><br><br>

### n19p4 外类比迭代: 正向反馈类比
`CreateTime 2020.04.10`

**简介:**   
　　因反向类比,并不能单独支撑起抽象不足的问题`参考19034`,所以本节将对正向反馈类比(以下简称:反向类比)进行迭代;

| 19041反馈类比对比 | 数据全面性 | 确切化速度 | 调用频率 | 抽象确切度 |
| --- | --- | --- | --- | --- |
| 正向反馈类比 | 好 | 慢 | 经常 | 常模糊 |
| 反向反馈类比 | 差 | 快 | 很少 | 常确切 |

| 19042 | 迭代分析 |
| --- | --- |
| 1 | 瞬时记忆新增一份matchAlg序列,并以此构建mProtoFo; |
| 2 | 用mProtoFo替代正向类比中的protoFo; |
| 3 | 用shortMatchModel替代正向类比中的assFo; |
| 注: | 原有瞬时序列和protoFo不变,只是新增matchAlg瞬时序列和mProtoFo; |

| 19043 | 训练计划 | 解说 |
| --- | --- | --- |
|  | A1(向3,距20,位8)->{mv-} |  |
|  | A2(向3,距20,位7)->{mv-} | 位置最善变 |
| A1:A2 | absA3(向3,距20)->{mv-} |  |
|  | A4(向1,距50)->{mv-} | 方向也易变 |
|  | A5(向1,距20)->{mv-} |  |
| A3:A5 | absA6(距20)->{mv-} | 距20不能吃 |
| A4:A5 | absA7(向1)->{mv-} | 向1不能吃(共经历2次) |
|  | A8(向1,距0)->{mv+} |  |
| A7:A8 | absA9(距0)->{mv+} | 反向类比,距0可吃 |

<br><br><br><br>

### n19p5 规划性训练3
`CreateTime 2020.04.10`
> 最近迭代了正反向反馈类比,本节重新规划规划性训练步骤,并逐步训练;

| 19051 | A组 | 目标:果可吃 `内类比,概念绝对匹配,时序识别,正向类比` |
| --- | --- | --- |
| A1 | 直投,直投 | absA1[pos38,dis0...]->{mv+} |
| A3 | 重启,移动,直投 | absA2[dis0...]->{mv+} |

**A组BUG:**
  1. 测试shortMatchFo生成后,影响到了原来的外类比,因为原有外类比,的assFo全部联想成了matchFo,分前进和后退两个解决方法:
    * **前进方法:** 即原有的外类比已经不重要,直接去掉,到后面能成功识别时,再反过来训练正向反馈类比;
    * **后退方法:** 想办法使shortMatchFo不要影响到旧有外类比方法;
    * **解决:** 选择前进,不走回头路,如下三条改动;
      - 更及时的持久化: 只要TIP即持久化生成fo和mvNode;
      - 识别更早就工作: TIR_Alg和TIR_Fo在第二次输入时,即开始工作;
      - 停掉原有外类比: 彻底停掉旧有的外类比调用;
  2. 空场景输入,导致mModel.matchAlg和matchFo变没,进尔导致外类比无法执行;
    * **分析:** 现在瞬时序列不清空,所以并不需要空场景输入;
    * **解决:** 因为不存在绝对的空场景输入,空场景也无法识别成,故不对空场景识别;
  3. A3移动直投后,生成时序为[飞,果],而联想到时序为[果,吃],二者无法全含匹配,因为此时lastItem为果,没有(吃),所以时序识别失败,导致外类比也无法进行,从而也无法类比出[dis0...]->{mv+};
    * **分析:** 因输出行为(吃)时,没有进行时序预测,我们视觉看到坚果时,无法预测变饱,但输出行为吃的时候,是可以预测的;
    * **解决:** 在输出行为时,也进行时序预测;
  4. 时序识别,因概念未去重,导致概念识别时用绝对匹配得到的结果,到时序匹配时却无法匹配上,有以下两种解决方法:
    * **方案1:** 在时序匹配时,checkAlgValid中,将alg用md5判断类比结果;
    * **方案2:** 三大改动:
      1. 对所有概念采用去重; T
      2. 只构建抽象概念,废弃具象; T
      3. 将概念识别绝对匹配去掉(因全局去重,变无意义); T

**A组改动:**
  1. 新的正向反馈类比,需要第一次输入的概念在TIP时就持久化; T
  2. 新正向反馈类比,需要第二次输入时,识别就开始工作; T
  3. 瞬时记忆中不仅放parent,也要放subAlg (parent是具象节点,无法被识别); T
  4. 在TIR_Alg_FromMem的checkItemValid中,不需要处理parent层了; T
  5. 不对空场景parentAlg进行识别,因其会导致A组BUG2问题; T
  6. 将瞬时记忆中的parent去掉; `暂不做`
  7. 支持多条ShortMatchModel的保留,以及反馈类比学习; T

| 19052 | B组 | 目标:远果不能吃 `反向类比,概念局部匹配,模糊匹配` |
| --- | --- | --- |
| B4 | 重启,投右,马上饿 | absA8[pos19,dis7...]->{mv-} |
| B6 | 重启,投左,马上饿 | absA10[...]->{mv-} |
| B7 | 返回,重进成长演示页,远投 | 识别matchA10(g255,size5),fuzzyA11 |

**B组BUG:**
  1. 右扔马上饿,左扔马上饿,概念无法局部匹配,因为`局部匹配`需要`全含抽象节点`,但`抽象`需要`反馈类比`,`反馈类比`又依赖`概念局部匹配`,形成死循环,无法切入;
    * **分析:**
      - 找切入循环的点,打破死循环,建议从类比切入;
      - 反馈类比,依赖TIP和识别预测,故尝试向微观一级;
    - **解决方案:** 在TIR中,写不依赖TIP的概念类比算法,步骤如下;
      - 第1步: 在TIR_Alg中,识别非全含但与inputAlg最相似的SeemAlg;
      - 第2步: 在TIR_Fo中,识别非全含但与inputFo最相似的SeemFo;
      - 第3步: TIP输入imv与SeemFo.mv同向时,调用正向反馈类比;
      - 第4步: 对inputFo和seemFo类比,并抽象absAlg;
      - 第5步: absAlg可作为今后的全含matchAlg概念识别结果;
      - 转至`n19p6`
  2. B4,测得外层死循环BUG,饿了输出行为`吃`,`吃`被识别预测为`mv+`,`mv+`又与当前任务相符,又转到`dataOut决策`,再输出行为`吃`,如此形成死循环;
    * **分析示图:** ![](assets/244_外层死循环BUG解决方式示图.png)
    * **示图说明:** 如图,在A-C之间,我们加入B,而B的执行结果,会影响到C之后的再决策,从而影响循环行为,从而中断死循环;
    * **例如:** 当我在用我每天都用的钥匙开门开不了的时候,行为如下:
      1. 第一种,我会再试一遍,看能不能开;
      2. 第二种,我不再试,因为被反思评价否掉了;
    * **转折:** 上图及解决方案为臆想,经打断点,并不会再次决策,而是直接输出上轮循环的actions,因为actions没清空导致;
    * **解决:** 将上一轮输出的actions及时清空,即可;
  3. 决策不记教训的问题: 接上BUG2,虽然清空了actions,并且每轮都会行为化,但还是在死循环,因为每次决策结果都是`吃`,然后再外层下轮转回来后,还是决策`吃`;
    * **分析:** 见BUG2_244示图,在第二轮行为化中,依据反向反馈类比的结果,可以通过评价方式知道远果无法吃,从而改变决策结果;
    * **实测日志:** B4投右,马上饿,在决策中发现MC_Value的M是Seem识别的0距果,导致MC直接就是一致的,所以直接决策行为`吃`了,但此时其实是远果,此问题有以下两个解决方案:
      * 方案一: MC_Value要针对protoAlg做,而不是matchAlg `30%,因为protoAlg杂信息太多`;
      * 方案二: 1.输出`吃`后, 2.下轮循环如果更饿了, 3.要先反向反馈类比(proto远距果 和 识别的0距果), 4.并发现远距导致吃不到, 5.进尔在下次识别远果时,不能再Seem识别为0距果; `70%,需迭代更多维度识别`;
    * **方案2分析:** 问题的核心在于,反向反馈类比的结果,并没有帮助到识别;
    * **比如:** 我在反向反馈类比时,已经知道距50,会导致{mv-},但还是直接Seem识别为0距果,导致决策时,不记教训;
    * **解决:** 从`反向类比`与`下轮识别`的协作上分析此问题的解;
    * **质疑:** 有些细微的特征(反向类比取得),并不被关注(下轮识别),而在决策时才会关注起来(RTAlg反思);`比如: 有点疤的苹果,用来打球踢时谁也不在乎有没有这个疤,但只要自己吃,才会去反思关注这疤痕`;
    * **反质疑:** 虽不被特别关注,但识别依然是首先的,只是我们需要扩展更全面的识别,并帮助决策;
    * **总结:** 本BUG是因为缺乏健全的反思 `转至n19p7`;

**B组变动:**
  1. MC_Value中RTAlg的反思识别,要支持Self方式;

| 19053 | C组 | 目标:学习飞行 `内类比` |
| --- | --- | --- |
| C8 | 重启,投上,飞上 | F1[飞上,距小] |
| C9 | 再摸... | 暂略 |

| 19054 | D组 | 目标:飞行行为 `决策,时序反思,概念反思` |
| --- | --- | --- |
| D10 | 重启,远投上,马上饿 |  |

| 19055 | 训练成功标志 |
| --- | --- |
| A1 | 识别概念: `----> 识别Alg success:A2(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0)` |
| A1 | 时序识别: `时序识别: SUCCESS >>> matchValue:0.333333 F3[(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0),吃1,]->M1{64}` |
| A1 | 内类比: `--------------内类比 (有无) 前: [(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0)] -> [吃1]` `-> 内类比构建稀疏码: (变 -> 无)` |
| A1 | 正向类比: `~~~>> 构建时序:[(速0,宽5,高5,形2.5,经207,纬368,距0,向→,红0,绿255,蓝0,皮0),吃1]->{64}` |
| A3 | 正向类比: `~~~>> 构建时序:F27[A26(速0,宽5,高5,形2.5,距0,向→,红0,绿255,蓝0,皮0),A1(吃1)]->M14{64}` |
| B4 | 反向类比: `~~~~> 反向反馈类比 CreateFo内容:F25[A9(距0),A1(吃1)]->M15{64}` `~~~~> 反向反馈类比 CreateFo内容:F26[A24(距44,经205,纬501)]->M18{-51}` |

<br><br><br><br>

### n19p6 扩展识别算法支持: MatchType_Seem
`CreateTime 2020.04.13`

> 简介: 原有`局部匹配A`要求`全含的抽象节点B`,而B又依赖`正向反馈类比C`得到,C又依赖A匹配的结果,所以形成死循环,`参考n1905-B组Bug1`,而此死循环必须找到一个切入点,来解决,本节重点从识别算法扩展相似度来解决此问题,以此形成死循环前的小循环,执行后,再切入死循环(以盘活),呈现出螺旋上升的信息处理过程;
> ##### MatchType共有四种,匹配度从低到高分别为:
> 1. Seem `仅相似`
> 2. Abs `抽象全含`
> 3. Fuzzy `模糊匹配`
> 4. Self `本身`

| 19061 | 方案简介 |
| --- | --- |
| 1 | TIR_Alg当self,fuzzy,abs都失败时,取相似seemAlg功能; |
| 2 | 并将其作为matchAlg返回,存到瞬时记忆的matchCache序列; |
| 注 | 当matchAlg组成的时序,预测价值与TIP真实发生相符时,会进行外类比; |

| 19062 | 使用范围说明 |
| --- | --- |
| 1 | 本文中SeemAlg仅供正向反馈类比使用; |
| 2 | 本文前MatckAlg&MatchFo继续供TIP(正向反馈类比)和TOR(MC)使用; |
| 优先级 | 概念匹配的优先级排序为:self > fuzzy > abs > seem; |
| 综述 | seemFo取每桢最优先概念进行构建,并用于正向反馈类比; |

```c
//19063 概念识别算法(相似版)伪代码;
TIR_Alg(){
    //1. ----Self
    //a. 自身是旧有节点,则匹配;
    //b. 逻辑说明: `不对结果建立抽具象关联` `要求是旧节点`

    //2. ----Fuzzy
    //a. 当abs有效时,可向具象找fuzzy结果,进行匹配;
    //b. 逻辑说明: `不对结果建立抽具象关联`

    //3. ----Abs
    //a. 全局找全含,最相似且全含的返回;
    //b. 逻辑说明: `相似计数并排序` `判断必须全含` `对结果进行抽具象关联`

    //4. ----Seem
    //a. 如果abs全含失败,则直接将最相似的返回;
    //b. 逻辑说明: `不要求全含` `不对结果建立抽具象关联` `不对结果进行fuzzy模糊匹配`
}
TIR_Fo_改动(protoFo){
    //protoFo参数的构建;
    //1. 原来是对瞬时记忆中protoCache构建protoFo进行识别;
    //2. 现改为对瞬时记忆中matchCache构建protoFo进行识别;
}
```

<br><br><br><br>

### n19p7 RTAlg反思:全面性迭代
`CreateTime 2020.04.17`

　　在规训3-B组BUG3中描述了因概念反思不够全面，导致一直在傻傻的输出`吃`这一行为，本文将围绕概念反思识别进行展开，对概念反思有更全面的支持，从而改善决策。
> **简写说明:**
> 1. M: MatchAlg
> 2. P: ProtoAlg

> #### 19071. 理性按照系统运转,从粗略至细繁结构化展开,分为三步,见以下表格:

| 理性三步 | 简述 | 算法 |
| --- | --- | --- |
| TIR识别 | 简单识别成功 | TIR_Alg_Mem |
| TOR决策 | 用识别的M和原Proto,在M和P间取各自优势; | MC_Alg&MC_Value |
| TIR反思 | 更加全面的反思识别RTAlg,并使决策更准确; | TIR_Alg_RT |

> #### 19072. 理性三步特性各有不同;

| 三者特性表 | 向性 | 数据量 | 涉及数据广度 | 作用(错误) | 数据(错误) |
| --- | --- | --- | --- | --- | --- |
| TIR识别 | 上 | 多 | 小 | 简单预测其用 | M |
| TOR决策 | 下 | 中 | 中 | 照顾所有信息 | M+P |
| TIR反思 | 上 | 少 | 广 | 全面反思评价 | 全面M |

> #### 19073. 原则分析:
1. **输入识别算法，向上注定丢失细节，原则为收束，粗略识别并预测。**
2. **反思识别算法，向下注定要照顾细节，原则为展开,是对原有粗略的见解进行详尽的展开和评价。**
3. **决策反思,可以模糊匹配,但不能无中生有;**  

　　决策中可以对稀疏码进行模糊匹配并评价，但绝对不能有任何一条稀疏码是无中生有的，如n19p2中的问题。

　　在此原则上，TIR_Alg_RT算法中的Fuzzy只能模糊匹配那些同区不同值的，而完全不同区不同值的要剔除掉。而Seem就更是无法支持（虽蛇咬十年怕绳，但我们明确知道绳子不咬人）。

　　而这个原则对确切单码（单特征）概念的要求更高，如：`(距0，经55)->{mv+}`，此处提出两种方式来对此抽象知识进行再确切，即得到：`(距0)->{mv+}`。
  1. 在输入TI反馈，反向反馈类比后，进行外类比抽象 `5%`。
  2. 在输出TO反思，做反思学习功能，进行外类比抽象 `95%`。

　　最终选择方案2，因为输入时要求收束和简单，输出反思时可以展开细节处理（见原则1和2）。

> #### 19074. 反思学习方案:
　　反思重组的RTAlg往往并不能绝对Self匹配，以下两个方案：

1. 方案1:![](assets/245_反思学习示图方案1.png)
2. 方案2:![](assets/246_反思学习示图方案2.png)
3. 总结:这两个方案都略复杂，首先A1A2可能只存在一个（虽然目前可能会有大于1个的情况），但下版本应用于现实世界时，可能只存在一个，而MC_Value中的C在图中，可能就是A3，此时其实是A3向下用模糊匹配找M特化值（距57），而不是从A57找A1和A2（方案1示图），因为P作为新节点，在网络中关联很少，如果我们把角度变一变，重新回到以C做索引上来，那么根据A3向下找到A1和A2（A3和A4因防重是同一节点），那么此时：具象的A1A2有杂乱信息（图中为Pos）干扰评价（比如Pos57在汽车前面，不能吃有危险），而抽象的A3又缺乏关键信息（图中为距57）。而在这二者之间缺乏一个即包含距57，又没有杂乱信息的节点，如下图：
4. ![](assets/247_反思RTAlg识别失败问题分析.png)
5. 至此，本节的全面性不应推给TIR,反而应该在MC_Value中解决,`转至n19p8`;

| TODO | STATUS |
| --- | --- |
| 1. MC_Value,需要操作(类比与反思)P并反思P的所有稀疏码; |  |
| 2. 如MC_Value操作P,那么TIR_Alg_RT,就需支持模糊匹配,否则很难反思成功; |  |
| 3. TIR_Alg_RT应该支持`Self`,`Fuzzy`,`全面的Match`,`Seem`四种识别(但不能无中生有,参考19073-原则3); |  |


<br><br><br><br>

### n19p8 结合`MC_Value`和`反向类比`分析决策失败的问题
`CreateTime 2020.04.18`

　　本文主要围绕MC_Value的运作，来分析决策失败的问题，并将问题的解决延伸到反向类比构建器中。说白了，就是使用者(TOR)发现问题，并向构建者(TIR)请求支持。

| 19081 | 反向反馈类比构建器迭代 (废弃,因反向类比成果未完整保留,参考n19p9) |
| --- | --- |
| 简介 | 在247图中，可知缺乏一个中间节点（图中绿色A3），此节点由反向类比支持。 |
| 示图 | ![](assets/248_反向反馈类比构建器迭代.png) |
| 解读 | 原来只是构建`ms->{mmv}`和`ps->{pmv}`,现在加入了sames而已。 |

| 19082 | TIR_Alg_RT迭代 |
| --- | --- |
| 简介 | 重组RTAlg的反思识别，围绕CAlg展开，`参考19074-回到以C做索引...`。 |
| 示图 | ![](assets/249_TIR_Alg_RT算法迭代示图.png) |
| 解读 | 如图RTAlg的重组分为两种,一种是加入C,一种是替换C中同区不同值的稀疏码; |
| 联想 | 第一种直接向具象fuzzy匹配,第二种需先抽象再具象fuzzy匹配; |
| 废弃 | 未使用好反向反馈的成果,比如反向反馈已经发现距离很重要,这里还在尝试重组,尝试找出距离到底是否重要,在TIR的成果未被TOR合理利用,这是无法容忍的,参考n19p9; |

| 19083 | 规划本文改动后的训练计划 | 获取方式 |
| --- | --- | --- |
| 1. 直投,直投 | [A1(距0,pos38,果...),吃]->{mv+} | 正比 |
| 2. 移动,直投 | [A2(距0,果...),吃]->{mv+} | 与A1正比 |
| 3. 远投,饿 | [A3(距57,果...),吃]->{mv-} | 与A2反比 |
| 4. 远投,饿 | 到MC_Value中,RTAlg=(P特化:距68 + C:A2)=(距68,果...),反思识别为A3; | 参考:19082 |
| 废弃 | 第3步已说明A2的0距非常重要,第4步却要重新发掘其重要性,破坏了一体一用的原则; | 参考n19p9 |


<br><br><br><br>

### n19p9 用"V+A"来解决反向类比与MC_Value的协作问题
`CreateTime 2020.04.18`  

　　在n19p8的解决方式中，发现对于TIR的反向反馈类比的构建改动会破坏其成果，且并没有被TOR的MC_Value很好的使用上。本节重点使用的方式来解决此问题。  

**概念说明:** V+A: 指一个稀疏码加一个概念，比如：红苹果。  


| 19091 | 代码规划 |
| --- | --- |
| 1 | 使用类似内类比`有无大小`的方式来给A做抽象（红、距57等等）。 |
| 2 | 写AINetUtils.absPorts_All(analogyType)方法，用来筛选取`有无大小`或者`反馈mv+,反馈mv-`或`普通抽象`。 |
| 3 | 以往只能从左索引向右取`有无大小`,现需支持从下向上取`analogyType`各类型的抽象 (可用datsSource来标识类型`源头`); |

| 19092 | analogyType枚举 | 子种类 |
| --- | --- | --- |
| 1. 正向类比抽象 | 即正向反馈类比构建的抽象。 | 1种 |
| 2. 反向类比抽象 | 即反向反馈类比构建的抽象 | 2种：价值+P、价值-S |
| 3. 内类比抽象 | 即内类比构建的抽象 | 4种：有H、无N、大G、小L |

| TODO | STATUS |
| --- | --- |
| 1. 支持用dataSource做当analogyType用; | T |
| 2. 内类比构建器中,用"有无大小"的值,来做dataSource; | T |
| 3. 反向类比支持"cPlus和cSub"的值,来做dataSource; | T |
| 4. 在AINetUtils中,支持根据type取不同的抽象pors; | T |
| 5. 在MC_Value中,不对所有稀疏码进行遍历处理,而是只对absPorts中,影响了价值正负的部分进行处理; | 转n19p10 |


<br><br><br><br>

### n19p10 MC_Alg算法迭代V3
`CreateTime 2020.04.20`

**略写概念说明:**
1. cP: 指cPlus,是AnalogyType_DiffPlus枚举值,表示反向类比导致mv+的节点;
2. cS: 指cSub,是AnalogyType_DiffSub枚举值,表示反向类比导致mv-的节点;
3. AF: alg&fo;

| 19101 | MC_Alg迭代v3分析 |
| --- | --- |
| 1 | 将原有M的cP/cS进行判断,并与C.cP协作,与CFo.cP协作; |
| 2 | 去掉M的RTAlg重组,反思直接由M的cP/cS来替代; |
| 3 | 重组RTFo反思识别时序,并取MatchRTFo的cS来判断M的cS是否需要修正(AF同时cS时,需要修正); |
| 4 | 反思评价中,cP/cS需同时在checkAlg和checkFo存在,才有效; |
| 5 | cP/cS的反思至少支持两层(当前+抽象一层),比如:我不能吃冷的,当雪糕冷,就不吃雪糕,不是雪糕不好吃,而是因为雪糕冷导致不能吃; |
| 6 | cP/cS需要分别对C和M进行反思 (但只对C满足,只对M修正); |
| 7 | 一切反思评价,都只是在引导M到C的加工; |
| 8 | cP/cS只是引个方向,真正要行为化的是其对应的概念或稀疏码; |

| 19102 | MC_Alg迭代v3伪代码步骤: `MC行为化算法包含两个功能` |
| --- | --- |
| 一 | **M到C的加工功能-第一部分** |
|  | 1. 反思评价前:判断有效性:`MC相同`或`mIsC`或`M.abs与C.con有交集`; |
| 二 | **M与C的反思评价功能** |
|  | 1. 对C的cP,进行满足 (需要在M中确认其缺失); |
|  | 2. 对M的cS,进行修正 (需要在CA/CF/MatchRTFo中确认也需要); |
| 三 | **M到C的加工功能-第二部分** |
|  | 1. 反思评价后:对C需满足的,M需修正的,进行行为化,比如消毒,洗干净,烤熟; |
| 注: | 1. MC行为化判断条件极简单(因为只单轮,复杂的交给递归); |

| 19103 | TOR三原则 |
| --- | --- |
| 原则1 | 决策主体为用非体,故不重组识别,更不进行学习 (不重组概念或时序); |
| 原则2 | 用法一:向性左下,决策只顺着向`具象与理性`关联找实现; |
| 原则3 | 用法二:向性右上,反思只顺着向`抽象与感性`关联找评价; |

| 19104 | 实例分析:`套用飞行距离的实例,来解析决策循环与MC行为化的工作`; |
| --- | --- |
| Q | 距0果,距57果,距43果,三者均无抽具象关联,那么我们就无法评价距离是好是坏,更无法做57->0的行为化; |
| A | 用决策递归解决,发现距0果无法匹配,则到时序中,发现距0果的cHav来自距10果,距10来自18,再29,再57 (虽然无法一轮成功,但智能体能够知道一直飞就到了); |
| > 注 | 因为内类比是对protoFo做,而protoFo中其实是放matchAlg,所以会有10,18,29,57这样的距离标志性值; |
| > 测试要求 | 需要测试内类比,看cHav有没有构建出[(距15,果...),飞,(距0,果...)]; |
| > 训练要求1 | 用飞的方式,两次吃到远果,并抽象出:[(距15),飞,(距0)]; |
| > 训练要求2 | 在决策飞时,可以飞,虽然距离是从43变成43,但仍可以决策为继续飞; |

| 19105 | MC_V3确定部分分析; |
| --- | --- |
| 准入条件 | mIsC的条件不变; |
| 满足C | C匹配上就加工M,匹配不上就转移C,转移失败就整体行为化失败; |
| 修正M | 完全是为了C,所以是否行为化稀疏码,行为化哪些稀疏码,都以C为基准; |

```java
//19106伪代码完整版

void alg_cHav(AlgNode c){
  //对概念找cHav;
  c = c.findCHav()
  if (c != null) {
    this.mc_v3(c,m);
  }else{
    //整体行为化失败;
    failure();
  }
}

void mc_v3(AlgNode C,AlgNode M){
    //1. 判断有效性
    if(mIsC){
        //2. C的mv+处理;
        Array cPlus = C.absPlus;

        //2.1 有效性判断1: 到CFo中,判断是否也包含此mv+,包含才有效 (C甜,也正是要吃甜的);
        Array cFoPlus = cFo.absPlus;
        if (cFoPlus.contains(cPlus.item)) {
            //2.2 有效性判断2: 再判断M是否包含此mv+,不包含才需满足 (C甜,M不能甜);
            Array mPlus = M.absPlus;
            if (!mPlus.contains(cPlus.item)) {
                //2.3 有效性判断3: 再到M中找同区不同值,对C稀疏码进行满足 (如C中含距0,而M中为距50);
                if (M.item.identifier == cPlus.item.identifier) {
                    this.mc_Value(M.item.value,cPlus.item.value);
                }else{
                    //2.3B 匹配不上,转移;
                    this.alg_cHav(C);
                }
            }else{
                //2.2B 无需处理;
                success();
            }
        }else{
          //2.1B 无需处理
          success();
        }

        //3. M的mv-处理 (排除掉刚已处理过的);
        Array mSubs = M.absSub - alreadyMSub;

        //3.1 有效性判断: 从CFo的具象两层中,到mv+中找同区不同值的稀疏码映射,(比如CFo或其具象中,吃热食物mv+,而M冷吃了还肚子疼mv-,则需修正);
        Array cFoPlus2Layer = cFo.absPlus + cFo.conPorts.absPlus;
        Array validM2CDic = (mSubs.identifier & cFoPlus2Layer.identifier);

        //3.2 逐一对M做修正 (如:行为化加热);
        for(id item in validM2CDic){
            this.mc_Value(item.cValue,item.mValue);
        }
    }else{
        //4. m不是c,直接转移;
        this.alg_cHav(C);
    }
}

void mc_Value(Value cValue,Value mValue){
  //对稀疏码value进行行为化;
}

```

| 19107_MC_V3模型图 |
| --- |
| ![](assets/250_MC_V3模型图.png) |
| 如图可见,修正M路径,与满足C路径,二者同代码结构,所以可复用一份代码; |

<br><br><br><br>

### n19p11 双向任务——决策对mModel全面支持
`CreateTime 2020.04.21`

　　在以往mModel在决策中仅支持MC，其中只是对mModel.matchAlg的支持，而对mModel.matchFo和价值预测都未支持，本节针对决策中对mModel的全面使用，做思考与设计变动。

**名词:**
1. 兄弟节点: 在反向反馈类比构建器中,构建的P和S一对节点,称为兄弟节点;

| 19111 | 双向任务 |
| --- | --- |
| 1. 左向任务 | **从内而外,比如饿了,想吃食物;** |
|  | 来源: 源自imv; |
|  | 解决: 有安全需求时,可以寻求安慰或者庇护所; |
|  | 算法: TOR_FromTOP() {//围绕MAlg满足CAlg;} |
| 2. 右向任务 | **从外而内,比如车快撞上我,我得躲开;** |
|  | 来源: 源自TIR的价值预测; |
|  | 解决: 在将被撞上时,应该躲避汽车; |
|  | 算法: TOR_FromTIR() {//围绕修正预测MatchFo;} |

| 19112 | 全面支持mModel分析; |
| --- | --- |
| 分析 | 右向任务,其实只是左向任务,以mModel.fo优先作为outModel解决方案而已 |
| 正反向 | mModel.matchFo与需求同向/反向,行为化区别为:满足H或打断N; |
| 代码 | 将TOAlgScheme行为化,分为:"正向H"和"反向N"两种 (目前仅支持H); |
| 正向H | 识别到坚果,预测只要吃,即可饱,并当前是饿状态,则优先维持matchFo; |
|  | 示例:[有皮果,去皮,无皮果,吃]->{mv+},看到有皮果,顺着时序完成即可; |
|  | 示例2: [果,吃]->{mv+},看到果,顺着吃掉即可; |
| 反向N | 识别到汽车,预测会撞,即会疼,为了不疼,优先防止matchFo的撞击; |
|  | 分析:需要找到共同抽象,如撞物体,再向具象找不被撞到的节点,并得出加工位置; |
|  | 示例:[车右,撞]->{mv-},根据车右撞cSub找cPlus的同级节点,并发现移动位置; |
|  | 结果:围绕已看到的节点M,进行cS找cP,并最终到稀疏码层解决问题; |

| 12113 | 双向 x 价值正负 = 四种决策模式: |
| --- | --- |
| a. 左正 | 饿了想吃东西,各种`发现M,并MC对比`,以解决问题; |
| b. 左负 | 如果不是自虐,不存在的 `由右负转来,TOP使用mModel.matchFo即可`; |
| c. 右正 | 看到0距果,吃就对了 `可转为左正,TOP优先使用mModel.matchFo即可`; |
| d. 右负 | 看到车撞过来,需要加工未来节点避免撞击,`用cS找cP`; |
| 注 | 只有a左正和d右负重要,bc为转换模式,b由d转来,c向a转去; |

| 12114 | 决策模式转换 |
| --- | --- |
| 右负转左正 | mModel为右负时,根据matchFo找兄弟cSFo,即左正 (修正M); |
| 左正转右负 | TOP.mcScheme触发,优先取用右正cPFo(mModel.matchFo优先)`与3同理`,其次根据cFo找兄弟cSFo,即右负 (两者与M对比满足C); |
| 左正取右正 | TOP.mcScheme优先取用mModel.matcoFo.mv+ `注:此条与2同理`; |

| TODO | STATUS |
| --- | --- |
| 1. 决策对mModel预测的全面支持 (随后再迭代); |  |
| 2. 对cPcS进行关联 (需思考下关联方式,兄弟节点); |  |
| 3. 思考下右负的SP表征方式与协作方式; |  |
| 4. 思考下左正的MCV3算法规则改动; |  |

<br><br><br><br>

### n19p12 回归测训4
`CreateTime 2020.04.24`

| 19121 | 分析 |  |
| --- | --- | --- |
| 1 | 将每一步生成的局部网络,记录笔记,以便训练到后面回朔; | T |
| 2 | 训练外类比,用远投-饿-返回重进-远投-饿,得到无坐标无距离坚果-{mv-}; |  |
| 3 | 训练外类比,用直投-移动-直投,得到无坐标距0果->{mv+}; |  |
| 4 | 改构建抽象mv时的方向索引初始强度,使其不至于太靠后; | T |

| 19122 | 训练步骤 | 结果 |
| --- | --- | --- |
| 1. 直投,飞右上,直投 | 正向类比 | [无坐标果,吃]->{mv+} |
| 2. 远投右,马上饿 | 反向类比,MC_V3 | cS[(距0)]->{mv-} |

| 19123 | 测试MC_V3 |
| --- | --- |
| 1 | MCV3算法的准入条件思考: 相似的评价; |
|  | 举例:已知狮子咬人,遇到老虎,会先认为其也咬人,除非它真的不咬;才会转变认知; |
|  | 分析: 是否在看到老虎时,与狮子相似,我们将其识别为狮子,或者对二者进行了类比抽象,得到猫科抽象概念; |
|  | 疑问: 问题在于,相似识别时,是否进行类比抽象? |
| 2 | MCV3算法的准入条件思考: 相似的评价2; |
|  | 举例:辛巴就不咬人,但会先识别它是狮子会咬,再在反向反馈中,认识到具象辛不咬人; |
| 3 | MCV3评价阶段思考:当MC评价时,优先以自身为准,后再以抽象为准; |
|  | 举例: 辛巴不咬人,但其抽象狮子咬人 (优先自身后抽象评价); |
| 4 | mIsC常失败问题分析; |
|  | mIsC,优先mIsC,然后判断m是不是C//抽具象不够,先产出合适的抽象,比如0距无位果,能吃,或者距区间(57)果,不能吃; |
| 5 | 套用实例,规划测试方法; |
|  | a. 使input相似匹配到"距0果",使决策方案采用0距无位果; |
|  | b. 行为化吃,并反向反馈为:距0Plus+,距57Sub-; |
|  | c. 下次输入距40果时,要优先相似匹配为:距57果; |
|  | d. 决策仍是距0果; |
|  | e. 二者MC后,发现需要解决距离问题; |
|  | 注: 关键就在于1c中,能否优先相似匹配为距57果;还是先让距>0的果进行抽象为无距果,再行模糊匹配? |
| 6 | 模糊匹配方案步骤: |
|  | c21. 距40相似匹配为距57果;预测不能吃; |
|  | c22. 马上饿,无法解决距离问题(因为还没学会飞),果然吃不到; |
|  | c23. 正向反馈,发现,无距果,不能吃; |
|  | c24. 再远投新的距35果,全含匹配到无距果,再模糊匹配到距57果; |
|  | c25. 预测不能吃…… |
|  | //这条线走着感觉,走不通,也走的没啥意义; |

| 19124 | MCV3跑通思考,训练步骤规划与规则变动分析; |
| --- | --- |
| 1 | 直投学会吃; |
| 2 | 远投,相似识别成距0果,并决策行为吃; |
| 3 | 反向反馈,发现(距47果xy>0),不能吃 `可将反向反馈二者进行共同抽象`; |
| 4 | 再投发现距53果 (识别成距47果),不能吃; |
| 5 | 决策MC中,需要距0果, `可将mIsC改为同层共同抽象也可` |
| 6 | 从M和C做MCV3,即可发现距离问题; |
| 注 | 只需要将反向反馈两个alg进行抽象,并mIsC改为同层判断,即可跑通; |

| TODO | STATUS |
| --- | --- |
| 1. 抽具象关联不够,导致mIsC老是false; |  |
| 2. 外循环,TORModel在上轮行为化成功时去掉其不应期; |  |
| 3. 是否考虑fo全局去重,来提高抽具象关联质量; |  |
| 4. 尝试先提取纯度抽象,再训练下一步的行为化; |  |
| > 比如先取得`右距43,无xy`果抽象,再对其进行,模糊识别为距0果,预测可吃,输出吃,反向反馈其因,发现距离>0不能吃, |  |
| > 下一次,再识别距57时,要模糊识别为距43果,而不再是距0 (测下); |  |

<br><br><br><br>

### n19p13 关联强度整理
`CreateTime 2020.04.25`

　　过去的初始强度都是拍脑门定的，比如：mv的方向索引初始强度为迫切度、稀疏码索引的初始强度为1、抽具象关联和组分引用的初始强度也为1。在其后使用中再以hebb+1来增强，而随着测试进展，这种当时无奈拍脑门的方式有了清晰的改进需求。本节将围绕此展开，整理网络的关联强度。

> **TIPS:**
> 1. 一朝蛇咬,十年怕绳,是描述概念的抽具象强度,而不是稀疏码被引用强度;  
> 2. 一个门派的功夫是否厉害,取决于他们的标志性功夫威力;  
> **略写说明:**
> 1. AFM: 指Alg & Fo & Mv
> 2. consMax: 指具象关联强度最强的一个;

| 19131 | 本文整理3种关联强度,4处改动 (其余保持原做法不变); |
| --- | --- |
| 一 | **抽具象关联** |
|  | *1. AFM的抽具象初始强度 = consMax + 1;* |
| 二 | **组分关联** |
|  | *2. alg.refPorts初始强度 = fo的cmv_p迫切度;* |
| 三 | **方向索引** |
|  | *3. 抽象mv方向索引的初始强度 = consMax + 1;* |
|  | *4. 具象mv方向索引的初始强度 = mv迫切度;* |
| 示图 | ![](assets/251_关联强度整理.png) |


<br><br><br><br>

### n19p14 决策-SP协作
`CreateTime 2020.04.28`

　　MCV3算法中的C源于TOP.mvScheme()，而M又源于mModel.matchAlg，这二者能组成cPcS的机率太小了，这种混乱导致很难有效的将反向反馈类比的成果应用好。本节针对此问题，对SP深度协作做处理，具体见文。

| 19141 | SP协作 |
| --- | --- |
| 说明 | cS必须有cP才有意义,而独立的cP也没有意义; |
| 例如 | M为生鸡蛋,我们会想到把鸡蛋变熟,而不是去胡思乱想吃面条这些无关的事; |
| 解读 | 即根据M.cS找到与其对应的cP,并转变为生熟的特征加工问题; |

| 19142 | 思考 |
| --- | --- |
| 1 | 将cPcS所处的两个时序，做兄弟节点 (类似mv基本模型的互指向)。 |
| 2 | MC算法的M和C要提前取自兄弟fo节点; |
|  | a. 左正则用C取兄弟cSFo,优先用C与M对比,次用cSFo对M对比,满足即可; |
|  | b. 右负则用M取兄弟cPFo,只要能够修正M实现cPFo,就可以; |

| TODO | STATUS |
| --- | --- |
| 1. 对cPFo和cSFo兄弟节点进行互指向; | T |

<br><br><br><br>

### n19p15 双向任务——决策对mModel全面支持2
`CreateTime 2020.04.29`

续n19p11 简写说明:
1. R+: 表示理性预测,Same同区MatchFo正价值影响 (即右正);
2. P-: 表示未预测的感性,Diff不同区MatchAlg负价值索引联想 (即左负)
  - 注:左负是为了由此找到兄弟左正;

| 19151 | 代码规划-双索引协作 |
| --- | --- |
| **左正** | A:TOP.mcScheme()先方向索引,后fo向具象时又以mModel为指引; |
|  | *>实例: 什么有安全感?在家时,想到家带来安全感,在玩想到朋友带来安全感;* |
|  | B:如果mModel指引没结果,则以左正fo找兄弟右负cSFo; |
|  | *>实例: 如想不到什么有安全感?则想什么不安全?风雨?由此房子有安全感;* |
| 示图 | ![](assets/252_TOP双索引示图.png) |
| **右负** | C:mModel右负,根据matchFo找兄弟左正cPFo; |
|  | *>实例:有人砍我?兄弟节点反杀即可;* |
|  | D:如果找不到cPFo,则根据matchFo.abs层,再尝试; |
|  | *>实例:反杀不了?向抽象想到有人对我造成威胁,找警察或者逃回家;* |
| **右正** | E:mModel右正,无需求,等左正时,取用mModel.matchFo指引,与A重复; |
|  | *>实例:手有坚果,但不饿,饿时?直接取matchFo吃掉即可;* |
| **总结** | 1. 右负,也要放到demand池去竞争,最终决策时,还是TOP转成左正; |
|  | 2. 右正,先在mModelManager中,也是待TOP决策时,转成左正; |
|  | 3. 所以以上,都是围绕左正的,全都是mModelManager对左正的指引罢了; |
| **综合** | 进行综合代码规划,将以上几种综合成mModelManager+左正; |
|  | 1. mModel存`右负->找兄弟节点解决,或者向抽象再找兄弟解决;` |
|  | 2. mModel存`右正->自身顺着解决,` |
|  | 3. TOP`左正->向具象,以mModel指引`,根据mModel.matchFo找右正/负,找不到时,则根据matchAlg `转至下表19152`; |

| 19152 | 应用mModel后,TOP四种工作模式 |
| --- | --- |
| 简介1 | 按照matchFo与demand是否同区,分两种,再按正与负分两种,共2x2=4种; |
| 简介2 | 一切提交给TOR,且汇于左正; |
| R+ | **同区预测正,将matchFo+作为CFo行为化;** |
| R- | **同区预测负,将matchFo-的兄弟节点作为CFo行为化;** |
|  | 难点: 对cutIndex进行处理,cutIndex是描述matchFo的截点的,如何用来定位兄弟节点中的cutIndex? |
|  | 举例: 已发生的无法修正 (比如车很近,我已来不及躲避,只好行为化为"做好撞击准备",或者将车击退); |
|  | 分析: 很近来不及躲是时序识别预测的事,与cutIndex无关;车有多近,采用何行为是matchFo对应的兄弟节点给出行为化的事,与cutIndex亦无关,而是与SubNode和PlusNode相关,因为无非是解决行车与智能体间的距离与方向问题; |
|  | 解决: a. 即使已撞上的车,也可以触发躲避,闪开; |
|  | b. 对sub和plus的修正,无需cutIndex; |
|  | c. 所以先不处理cutIndex,而是以指定subNode和plusNode来替代; |
| P+ | **非同区无预测,mv方向索引找正价值解决方案;** |
|  | ![](assets/253_DMA正示图.png) |
| P- | **非同区无预测,mv方向索引找负价值的兄弟节点解决方案;** |
|  | ![](assets/254_DMA负示图.png) |

| TODO | STATUS |
| --- | --- |
| 1. 在mModel中,添加mAlgIndex属性值; |  |
| 2. 思考下左正的MCV3算法规则改动; |  |


<br><br><br><br>

### n19p16 决策四模式从TOP到TOR
`CreateTime 2020.05.12`

　　在n19015中，TOP应用mModel后，有了四种工作模式，分别为：P+、P-、R+、R-。而TOP最终是要提交给TOR做行为化的，本文将应对此变化，对TOR进行迭代。

　　思维控制器按照IO（入与出）、RP（理感性）、SP（正与负）共分为2³=8种模式。

**略写说明:**
1. SP: Sub&Plus
2. R: 理性
3. P: 感性
4. pFo: plusFo
5. pcFo: pFo.conFo

**19161_TOR四模式的参数**
1. 所有`R`都会有cutIndex (即是理性的,则有已发生的部分);
2. 所有`负`都有SP (即是负的,则指定了正的方向);

| 模式 | cutIndex | cFo | S | P |
| --- | --- | --- | --- | --- |
| R+ | √ | √ |  |  |
| R- | √ | √ | √ | √ |
| P+ |  | √ |  |  |
| P- |  | √ | √ | √ |

**19162_TOR四模式分析**

| 模式 | 类别 | 感性 | 实例 | TOR做什么? | 代码 |
| --- | --- | --- | --- | --- | --- |
| R+ | 理正 | 思->好 | 饭张口,衣伸手 | 好事将至->顺应行为 | fo.out |
| R- | 理负 | 恐->怒 | 车撞躲,狗咬打 | 坏事将至->避免行为 | sp |
| P+ | 感正 | 盼->喜 | 饿做饭,困睡觉 | 期盼好事->行为实现 | fo全部mc |
| P- | 感负 | 忧->悲 | 疼揉揉,累歇歇 | 担忧坏事->行为避免 | sp |

**19163_代码初步规划**

| 模式 | 代码规划 |
| --- | --- |
| R+ | 对cFo的cutIndex之后的isOut部分,进行顺应,比如饭来了,输出吃; |
| R- | 对cutIndex之后的plusNode中部分进行实现,比如躲开车; |
| P+ | 对cFo所有alg,进行逐一行为化 (即TOR一直在跑的左正模式); |
| P- | 对plusNode中部分进行实现,比如疫情不出门了; |

**19164_TOR.R+模式模型**

| 示图 | ![](assets/255_TOR的R+模型.png) |
| --- | --- |
| 说明 | 对matchFo下一元素进行isOut判断,true则输出,false则等待; |
| 缓存 | 对outModel的保留先不做,靠mModel短时来试下能否支撑下来,不行再做; |

**19165_TOR.R-模式模型**

| 示图 | ![](assets/256_TOR的R-模型.png) |
| --- | --- |
| 说明 | 对matchFo下一元素进行先isSP,后isOut判断; |
| 缓存 | 对outModel的保留先不做,因为下轮循环,再一次重新决策即可; |
| 循环 | 下轮循环,会重新决策,其可能指向另外一个R+或R-; |
|  | 如: 上帧为躲开车,下帧可能就直接预测mv+了,顺势开心起来即可; |
|  | 如: 上帧为躲开车,下帧因车位置变化,转向另一个R-,可能面向另一个方向躲; |

**19166_TOR.P+模式模型**

| 示图 | ![](assets/257_TOR的P+模型.png) |
| --- | --- |
| 说明 | 对matchFo首个元素进行cHav行为化; |
| 缓存 | 使用outModel结构化缓存,做简单的计划能力; (废弃) |
|  | 比如做饭的主线计划,含支线买菜,洗菜等; |
| cutIndex | 用outModel来提取cutIndex,即任务做到什么进度了; (废弃) |
| 外循环 | 无论是否存outModel,下轮循环都重新识别,并极可能转至R+ |
|  | 因为已做过的存在瞬时序列中,压根不需要outModel存cutIndex; |

**19167_TOR.P-模式模型**

| 示图 | ![](assets/258_TOR的P-模型.png) |
| --- | --- |
| 说明 | 对plusFo首个元素直接进行行为化实现; |
| 转移 | 在下轮时,可能转移至R+或者P+; |

| 19168 | 转移分析 | 转移时协带数据 (保持激活节点) |
| --- | --- | --- |
| R+ | 转R+ | 下面三者,都希望转为R+; |
| R- | 转R-/R+ | 实现的pFo与其pcFo,带给R预测 (如车将撞车,已成功躲开); |
| P+ | 转R+ | 将mFo带给outModel,后用于下轮的预测 (如炒菜买到菜了); |
| P- | 转P-/R+ | 实现的pFo与其pcFo,带给R预测 (如累已买到水); |

| TODOLIST | STATUS |
| --- | --- |
| 1. 一次只对一帧进行行为化; |  |
| 2. 行为化成功时,添加到outModel短时记忆中; |  |
| 3. TIR_Fo识别算法,优先从outModel中,做更及时的预测; | 废弃 (就存outModel中即可) |
| 4. 真正向下帧跳转,发生在事实发生之后 (即新的input匹配到); |  |

<br><br><br><br>

### n19p17 决策四模式之OutModel短时记忆
`CreateTime 2020.05.15`

| 19171 | 短时记忆分析 |
| --- | --- |
| OutModel | 1. outModel作为TO的短时记忆模型,保证了外循环下任务的持续; |
|  | 2. R+R-P+P-四者都要(废弃)->存到outModel->一次仅处理单帧; |
|  | 3. R+和P+存到outModel中,而R-和P-只是推进器,将R/P+推进一帧而已 (如买菜为做饭); |
| 协作示图 | ![](assets/259_短时记忆协作示图.png) |
|  | 1. TOP借助mModel更好的跳转到TOR; |
|  | 2. TIR借助outModel更及时的预测到TIP (判断当前瞬时与下一帧匹配); |
| OutModel模型 | outModel结构化模型不变,检查下,是否具备: |
|  | 1. score: 评分,用于实时竞争; |
|  | 2. status: 状态,用于当前是否`不应期`或`长期激活`的判断; |
|  | 总结: 同时`长期激活`状态多个子方案,会以score互相实时竞争; |

| 19172 | OutModel模型 |
| --- | --- |
| 模型 | ![](assets/260_OutModel模型图.png) |
| 说明 | 蓝色为每一次负预测时,向下子实现; |
|  | 黄色为,每一次子实现后,回归父级移动至下帧; |

| TODO | STATUS |
| --- | --- |
| 1. 对TOP的另外三个模式集成TOFoModel; | T |
| 2. 对TOR.R+集成TOFoModel; | T |
| 3. TOR中isOut=false时处等待,改为cHav行为化 (包括R+,P+); | T |
| 4. 行为化成功(input下帧匹配)时,跳转至下帧 (更新status为Finish); |  |
| 5. TOR中,isOut=true时直接status=ActYes | T |
| 6. 对行为化toAction,评价失败时status=ScoreNo; |  |
| 7. 对行为化toAction,跳转时outModel添加subOutModel; | T |
| 8. 将toAction.cHav方法,改为多轮输出行为化结果,而非单次; | T |
| 9. 调用TOAction.`_SP,_P,_GL,_Hav,_Fos,_Fo`时,实例一个outModel当参数传进去; | T |
| 10. 在下面传给GL时,生成TOValueModel; | T |
| 11. 传给_Alg时,生成TOAlgModel; | T |
| 12. 在方法执行中status变化的,重新对status赋值即可; | T |
| 13. 在_Alg方法转移时,对TOAlgModel生成actionFoModel; | T |
| 14. 每一次subAct时,都直接进行输出 (中断只有两种情况,理性的即行为化失败,感性的即评价失败); | T |
| 15. 每次获取到一个新的fo时,都要尝试进行评价,以中止此subOutModel; |  |
| 16. 对单帧Finish的,要在下轮input传回判断是否符合要求,并跳转至下帧; |  |

<br><br><br><br>

### n19p18 决策四模式之行为化迭代
`CreateTime 2020.05.17`

本节重点针对四模式后的行为化,做分析,以推进TOR四模式代码完善;

| 19181 | TOR输出:`等待`分析 |
| --- | --- |
| 1 | 所谓等待,只是不必输出行为即自出现的节点 (等事实发生,行为化成功只是无行为); |
|  | a. 如果发生成功,则cutIndex向下帧跳转; |
|  | b. 如果未成功,则反向类比出原因,并对S修正,输出行为; |
| 举例 | 每到中午,铃声一响,就有食物投过来,当有一天未响,也未投食,决定自己去手动下铃声,看会不会有食物投进来; |
| 总结 | 在TOR中,常默认返回`等待`,但如果等待无果,则反向类比,并下轮由+转-,进行行为化; |
| 0521更新 | 无法判定什么是`等待无果`,是还没来?还是已不可能来?所以此问题重新分析; |
| 分析 | 默认调用cHav行为化,而不是等待,参考19182原则4; |

| 19182 | 原则分析 |
| --- | --- |
| 原则1 | 行为只做两件事,`一是改变负预测`,`二是顺应正预测`; |
| 原则2 | 决策只是将负预测变为正预测或避免负 (负预测触发行为,而非平白无故的主动加工SP); |
| 原则3 | P怕错过,但S不怕,S本来就是发生后,用来修正的,所以S不怕已发生,只怕无法修正; |
| 原则4 | NormalFo仅标示做事的方式,cHav才标示如何出现; |

| 19183 | 代码步骤分析 |
| --- | --- |
| 1 | 加工前,已负预测的,找出已发生的S,并找出能否通过P来修正; |
| 2 | SP加工,isOut=true时,可直接输出或不输出,来解决; |
| 3 | SP加工,isOut=false时,可转至cHav或cGL来做行为化; |
| 4 | 加工后,会变成正预测,则顺着输出isOut为true的行为; |

| 19184 | 实例 |
| --- | --- |
| 1 | 以往的行为化,是对SP的主动加工,但行为化可能并非平白无故的主动对SP进行加工; |
| 2 | 比如曾有猫咪A过来舔我,猫咪B过来抓我,现在看到一只猫C; |
| 3 | 我所需的行为可能并不是不问青红皂白,将猫咪A变成猫咪B,或者将抓变成舔,而是看到猫B时,预测抓我,再进行SP行为化; |
| 4 | 其实就是将负预测,加工成正预测(或者避免负); |
| 5 | 比如猫A无法加工成B,抓的行为也无法加工,所以再循环转移SP,最终远离猫咪B,或者用衣服将自己裹更严实; |

| TODO | STATUS |
| --- | --- |
| 1. TOR.P+放至outModel后,如何判定首帧未自动出现,后转至行为化(参考原则4) |  |
| 2. 将TOR的默认行为由`等待`改为`cHav行为化`,参考19182原则4; |  |
| 3. 检查下内类比方法,有没有做到非常全面的aIndex和bIndex之间构建cHavFo; |  |
| 4. 考虑将TOAction._SP()方法中,对_GL的判断,向抽象和具象延伸,而避免调用_Hav(); |  |


<br><br><br><br>

### n19p19 P+模式重新改回递归
`CreateTime 2020.05.21`

　　以往的P+模式，以方向索引到解决方案.conPorts和MatchAlg.refPorts之间取交集，但这方式最大的问题在于，其不如递归来的更加发散，本文针对此来做迭代。

| 19191 | 原则分析 |
| --- | --- |
| 相对1 | 极度收束的短时记忆,极度发散的网络联想; |
| 相对2 | 四海皆准的抽象方案标示所有帧,递归的cHav联想推进单帧; |

| 19192 | P+模型 |
| --- | --- |
| 示图 | ![](assets/261_P+模型.png) |
| 说明 | 1. 左有input输入,右有方向索引; |
|  | 2. 方向索引到absFo后,直接提交到outModel,并对首元素进行cHav行为化; |
|  | 3. cHav中,会与matchAlg协作,找出合适的cHavFo; |
|  | 4. 可能再次转移,再次cHav找subCHavFo,如此递归直至行为输出解决了此帧问题; |

| 19193 | 原则2:normalFo和innerFo的区别是什么? |
| --- | --- |
| cHav构建 | ![](assets/262_内类比cHav构建图.png) |
| 说明 | 由图可见 |
|  | 1. normalFo是感性的确切化 (fo->mv); |
|  | 2. innerFo是理性的确切化 (a1,a2...->a3); |
| 结果 | 所以用normalFo做流程控制,用innerFo做单帧推进是正确的; |

| TODO | STATUS |
| --- | --- |
| 1. 将TOR.P+的取交集,转为方向索引取抽象时序提交至outModel,并逐帧cHav递归行为化; |  |
| 2. 在TOAction.cHav()中,添加不依赖SP的发散联想cHav,参考19192示图; |  |


<br><br><br><br>

### n19p20 决策与外循环深度集成OutModel
`CreateTime 2020.05.30`

| 19201 | 决策与外循环 |
| --- | --- |
| 示图 | ![](assets/263_决策与外循环.png) |
| 说明 | 1. OutModel输出actions到行为后,递归回TOP; |
|  | 2. TOP再根据DemandManager和思维活跃度,继续下轮循环; |
|  | 3. 允许subOutModel将自己上报给DamandModel,并在下轮循环时,直接判断行为有效性; |

| 19202 | 外循环输入时的收集判定匹配 |
| --- | --- |
| 示图 | ![](assets/264_外循环输入时的收集判定匹配示图.png) |
| 说明 | 1. 新的输入帧,收集Demand下所有ActYes和Runing,进行匹配判断; |
|  | 2. 匹配后,进入推动决策进度方法,参考19203; |

| 19203 | 外循环推动决策进度 |
| --- | --- |
| 示图 | ![](assets/265_外循环推动决策进度.png) |
| 说明 | **以fo.Finish找fo.base触发:Alg/Value/Demand,三个流程如下** |
|  | 1. 左侧为Alg匹配时推动流程; |
|  | 2. 右侧为当Value完成时推动流程; |
|  | 3. Demand完成时推动流程未画出,因为Demand完成为总流程完成; |
|  | 总结:以上各种流程可以统一写到一套方法代码中; |

<br><br><br><br>

### TODOLIST

| TODOLIST |  |
| --- | --- |
| 20200414 | 将ShortMemory与ShortMatchManager合并; |
| 20200415 | TOP对MModel的使用1: TOP_FromTIR()时,mModel.matchFo预测可解决需求,不要直接抵消,而是推进matchFo的完成,直至TIP输入mv+,正式完成任务; |
|  | 例:预测将吃到饭,但并没有吃到,直接抵消并不明智,我们需要看时序后面会发生什么,比如我需要去盛饭,或者什么都不用做,等着即可,但就算是等着,也不能抵消掉任务,而是此任务的决策模型中已有在完成中... |
|  | 注:此任务代码有可能本来就是这样做的,需要回溯以往代码; |
| 20200415 | TOP对MModel的使用2: TOP_FromTIP()时,要结合mModel来做fo解决方案的选择; |
|  | 例:石头将砸到自己,是优先考虑躲避当前的石头,而不是回想上次为了安全躲到防空洞,这种不相关的解决方案; |
|  | 注:此任务代码有可能本来就是这样做的,需要回溯以往代码; |
